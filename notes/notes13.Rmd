---
title: "13. Normal Forms"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

```{r, include=FALSE, message=FALSE}
source("../funs/funs.R")

food <- read_csv(file.path("..", "data", "food.csv"))
food_prices <- read_csv(file.path("..", "data", "food_prices.csv"))
diet <- read_csv(file.path("..", "data", "food_diet_restrictions.csv"))
```

## Data Normalization

Database normalization is a concept from database theory that describes a
hierarchy of increasingly strict guidelines for how to structure data across
multiple tables. The benefits of tables that are more normalized is that they
are less error-prone, more efficient, reduce redundancies, and improve data
integrity. From a data science perspective, working with normalized tables 
often leads to cleaner code and analyses. However, it can more difficult to
collect the original data in a normalized format. Further, while it is usually
more straightforward to write code based on normalized formats, it often 
involves more steps than working with a less normalized set of tables. So, like
everything, there are always benefits and tradeoffs, which makes it even more
important to learn about the concept so that you can make good choices in your
own work.

In these notes I will describe three increasingly strict types of data
normalization: first normal form, second normal form, and third normal form.
My recommendation when doing data science is to try to always respect the first
normal form at all times. The other two (stricter) forms of normalization are
useful to consider, but do not feel the need to following them dogmatically,
particularly during the initial data collection process.

## First Normal Form (1NF)

The first normal form, often abbreviated as 1NF, simply requires that each
value (individual element) in a table is an individual element. Formally, the
technical definition of 1NF only disallows hierarchical structures such as
"tables whose elements are themselves inside tables". While it is possible to
have what are called "nested tables" in R, we have not seen these in this
course and will not have a need to create them (they come up frequently in
DSST389, though). In the context of data science practice, however, the 1NF
rule typically corresponds to the stricter condition that no value should 
contain more than one individual piece of information. This is the definition
that we will use in this course.

For an example of a dataset that does not respect 1NF, let's look at the data
from the fifth anti-pattern discussed in the recent slides:

```{r, echo=FALSE}
ap <- read_csv("../data/antipattern05.csv")
ap
```

The data table above is not in 1NF because it has more than one type of food in
some of the values in the last column. In order to create a 1NF version of the
data, we would need to re-write it like this:

```{r, echo=FALSE}
ap |>
  mutate(fav_foods = stri_split(fav_foods, fixed = "; ")) |>
  unnest(c(fav_foods))
```

The new form of the data now respects the 1NF rules. We will see in a future
set of notes the code do perform this operation.

## Second Normal Form (2NF)

Describing the second normal form requires defining a four concepts from
database theory:

- **superkey**: a collection of one or more features whose values uniquely 
define an observation in a table
- **candidate key**: a minimal superkey, that is, a collection of one or more
features whose values uniquely define an observation in a table but for which
no proper subset would uniquely define an observation 
- **primary key** a specific choice from the set of candidate keys to serve as
a primary identifier for an observation; other candidate keys are called 
*alternative keys*
- **foreign key** the presence of one table's primary key in another table for
which it is not a primary key; this forms what is called a *relation*

A table satisfies the second normal form, or 2NF, if it satisfies the
conditions to 1NF and any feature outside of a candidate key depends on the
entire value of a candidate key. An example will make this much more clear.
We will primarily work with tables that have only a single option for the 
candidate key.

Consider the following version of the U.S. cities data we have been looking
at this semester. It contains only a subset of the columns and the rows have
been randomly re-arranged to make it more clear the relationships between the
features.

```{r, echo=FALSE}
us <- read_csv("../data/us_city_population.csv")
us <- us |>
  select(city, year, population, lon, lat) |>
  slice_sample(prop = 1)
us
```

The sole candidate key, and therefore primary key, in this data table consists
of the first two features, 
$\{\texttt{city},\, \texttt{year}\}$. Knowing this, consider how the other
features in the data depend on the values in the candidate key. The value of the
`population` feature depends on knowing both the city and the year; however,
the longitude and latitude of the city in this data set only depend on the
city. The city does not change locations over time, and so the `lon` and `lat`
columns only depend on `city` and not `year`. This is a bit more clear perhaps
if we look at a single city:

```{r, echo=FALSE}
us |>
  filter(city == "Richmond, VA") |>
  arrange(year)
```

As you can see above, the data contains a lot of duplicated information 
because it repeats the location of Richmond, Virginia every year even though
the location does not change.

To make the data respect the 2NF, we need to create two different tables. One
would contain information about the original candidate key, which is just the
result of removing the `lon` and `lat` features:

```{r, echo=FALSE}
us |>
  select(-lon, -lat)
```

And having another table with a single row for each city providing its location:

```{r, echo=FALSE}
us |>
  select(city, lon, lat) |>
  unique() |>
  arrange(city)
```

It is almost always the case that converting a dataset that does not respect
2NF into one that does respect 2NF involves split data into multiple tables.

A useful concept is the idea of a *functional dependency*, which occurs when
one feature in the dataset is dependent to the value(s) of other feature(s).
A fancy way of stating the second normal form is to say that no non-prime
feature has a functional dependency on a proper subset of any candidate key.

## Third Normal Form (3NF)

The next and final most strict normal form is called, somewhat unsurprisingly,
the third normal form, or 3NF. A table is in 3NF if the table is in 2NF and
all of the features that are not part of any candidate key depend *only* on the
entire candidate key. At first this seems a lot like a redundant version of
2NF, but there is a subtle difference. Let's see an example to understand the
distinction.

We will take a portion of the `food` table combined with part of the `diet`
table that we saw in one of the previous notebooks.

```{r, echo=FALSE}
food |>
  select(item, food_group) |>
  left_join(diet, by = "food_group") |>
  select(item, food_group, vegan)
```

This table satisfies the 2NF conditions but not the 3NF conditions. To see why,
notice that the only candidate key in this table is $\{\texttt{item},\}$.
Each of the other
features, `food_group` and `vegan`, depend on the value of the entire candidate
key (there is only one feature in the candidate key, so we get 2NF almost for
free). However, there is still some redundancy in the data, because the column
`vegan` is entirely dependent on the column `food_group`. So, there is a
functional dependency between the last feature and a feature that is not part
of the primary key, hence 3NF does not hold. To fix the problem, we just return
to the original two data tables:

```{r, echo=FALSE}
food |>
  select(item, food_group)
```

And:

```{r, echo=FALSE}
diet |>
  select(food_group, vegan)
```

There is one commonly seen type of even more strict normalization called 
Boyceâ€“Codd normal form, abbreviated as either BCNF or 3.5NF. It involves a
very small change to the the 3NF conditions that are difficult to describe 
and not particularly important in Data Science. It involves some clever 
thinking to even come up with an example of a data set that satisfies 3NF but
does not satisfy BCNF. There are even higher forms, including 4NF, 5NF, and 6NF,
that are largely only of theoretical concern and often not even discussed in a
full-semester database course. So, we will end our lesson here. 

As a final note, know that the 3NF condition was re-defined by Hadley Wickham
(the author of packages such as **ggplot2** and **dplyr**) using the concept of
"Tidy Data". If you see this term or its variants in data science literature or
packages, know that it relates to the data normalization concepts presented 
here.

## Movies Data

In class next week we will be working on two notebooks that investigate a 
dataset about the top 100 grossing films for each year from 1970 to 2019. The
data comes from IMDb. I have (almost) organized the data into the 3NF format
across four different tables. Let's get familiar with the data here before
attempting the somewhat difficult questions in this week's notebooks.

```{r, message=FALSE}
movies <- read_csv(file.path("..", "data", "movies_50_years.csv"))
m_genre <- read_csv(file.path("..", "data", "movies_50_years_genre.csv"))
m_people <- read_csv(file.path("..", "data", "movies_50_years_people.csv"))
m_color <- read_csv(file.path("..", "data", "movies_50_years_color.csv"))
```

The movies dataset contains one row for each movie. Most of the variables are
fairly straightforward; you can see the units by opening the data dictionary.
Three variables concern the movie's theatrical poster. These given the average
brightness (average pixel intensity), saturation (are the colors bright or
washed out / white), and complexity (a cartoon image would have a low
complexity; lots of text or icons would have a high complexity).

```{r, message=FALSE}
movies
```

A second dataset gives more detailed information about each poster by indicating
how much of a poster is of a certain color. If you want to look at the movie
poster itself, just search for a film on [IMDb](https://www.imdb.com) and
search for the film. The poster is the first image on the film's page.

```{r, message=FALSE}
m_color
```

We also have a dataset of movie genres. The data structure is straightforward,
but needs to be kept in its own table because a single movies can be assigned
to multiple genres.

```{r, message=FALSE}
m_genre
```

Finally, we also have a dataset of people associated with each film. We do not
have a lot of metadata about the people, but I have added a prediction of each
person's gender based on U.S. Social Security records. These are not always
correct (there is a confidence score included as well) but are useful for some
aggregate analyses. Note that the feature `rank` indicates the order in which
the names were listed in the IMDb dataset, which sometimes indicates the 
relative importance of the actors to the film.

```{r, message=FALSE}
m_people
```

There is a lot to unpack with these datasets. We will need to make use of the
many methods we have learned so far this semester to make sense of the data.

## Homework

For homework, complete the following three questions:

1. Identify the candidate keys for each of the four movies tables above. Do any
tables have the same exact candidate keys? Do any tables have multiple candidate
keys?
2. Describe which candidate keys appear as foreign key(s) in other tables.
3. Identify the two violations of 3NF in my data. Is this also a violation of
2NF? How would you normalize the data to put it properly into 3NF?

Try not to worry too much about the meaning of each column (we will have a
full data dictionary when we work on these in class); just do your best to
answer the questions given your intuition.

## Homework Answers

For Q1, the candidate keys for the tables are as follows:

- **movies**: $\{\texttt{year},\, \texttt{title}\}$
- **m_color**: $\{\texttt{year},\, \texttt{title}, \texttt{color}\}$
- **m_genre**: $\{\texttt{year},\, \texttt{title}, \texttt{genre}\}$
- **m_people**: $\{\texttt{year},\, \texttt{title}, \texttt{role}, \texttt{rank}\}$

Note that we need both the year and title to describe a film because some films
were re-made in a different year. None of the tables have alternative cadidate keys
when looking at the full dataset.

For Q2, the movies candidate key appears on every other table as a foreign key.
No other candidate keys appear on other tables.

For Q3, there are two violations. In the **m_color** table, there is a dependency
between `color_type` and `color`. The latter is a proper subset of the candidate
key and therefore breaks 2NF. The second violation occurs in **m_genre**. The 
columns `gender` and `gender_conf` are functions of the column `person`. This is
a violation of 3NF but not 2ND because `person` is not part of the candidate key.
